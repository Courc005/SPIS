{"cells":[{"cell_type":"markdown","metadata":{},"source":"Linear Prediction\n=================\n\n"},{"cell_type":"markdown","metadata":{},"source":["## The importance of linear prediction and coding\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Linear prediction and coding (LPC) play a crucial role in digital signal processing (DSP) and cognitive science. By making predictions about the future based on past experiences and sensory input, our brains are able to anticipate and respond to changes in the environment in a more efficient and adaptive manner. This process of prediction allows the brain to generate expectations about what will happen next, which helps to reduce surprise and uncertainty. Linear coding, on the other hand, involves representing sensory input in a way that can be efficiently processed and used for making predictions. By encoding sensory information in a linear fashion, a DSP system (or the brain) can extract relevant features and patterns from complex input and build a more accurate model of the world.\n\nIn computational cognitive science, LPC helps to simulate the optimization of the brain's internal models and improve its ability to make accurate predictions about the environment. This, in turn, allows for more efficient action selection and decision-making, ultimately enhancing the brain's ability to adapt and survive in a constantly changing world.\n\nIn [DSP](https://www.mathworks.com/help/releases/R2024a/signal/signal-modeling-subcategory.html), LPC is used for signal modeling. It provides also an excellent introduction to parametric spectral estimation. For more information about LPC in DSP, please refer to \n\n-   [Matlab DSP Toolbox Examples](https://www.mathworks.com/help/releases/R2024a/signal/signal-modeling-subcategory.html?s_tid=CRUX_topnav)\n-   Linear Prediction as a Representation in [https://speechprocessingbook.aalto.fi/Representations/Linear_prediction.html](https://speechprocessingbook.aalto.fi/Representations/Linear_prediction.html)\n\nBelow we provide a short introduction to \n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Speech Decomposition with Source Filter Model\n\n"]},{"cell_type":"markdown","metadata":{},"source":["In this example, we decompose a speech signal into its source $e[n]$ and filter components $a_k$, following the LPC model \n\n$$\ns[n] = e[n] + \\sum_{k=1}^M a_k s[n-k]\n$$\n\nWe'll first use the traditional method to estimate the LPC filter, and\nthen we'll use our differentiable LPC to do end-to-end decomposition.\n\nAgain, let's first import the necessary packages and define some helper\nfunctions.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\nimport torch.nn.functional as F\nimport torchaudio\nimport math\nimport numpy as np\nfrom torchaudio.functional import lfilter\nimport matplotlib.pyplot as plt\nfrom typing import Optional, Tuple, List, Union\nfrom IPython.display import Audio\nimport diffsptk"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def plot_t(\n    title: str,\n    ys: List[np.ndarray],\n    labels: List[str] = None,\n    scatter: bool = False,\n    axhline: bool = False,\n    x_label: str = \"Samples\",\n    y_label: str = \"Ampitude\",\n):\n    for y, label in (\n        zip(ys, labels) if labels is not None else zip(ys, [None] * len(ys))\n    ):\n        plt.plot(y, label=label) if not scatter else plt.scatter(\n            np.arange(len(y)) + 1, y, label=label\n        )\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)\n    if label is not None:\n        plt.legend()\n    if axhline:\n        plt.axhline(y=0, color=\"r\", linestyle=\"dashed\", alpha=0.5)\n\n\ndef plot_f(\n    ys: List[np.ndarray] = None,\n    paired_ys: List[Tuple[np.ndarray, np.ndarray]] = None,\n    ys_labels: List[str] = None,\n    paired_ys_labels: List[str] = None,\n    sr: int = None,\n):\n    if ys is not None:\n        for y, label in (\n            zip(ys, ys_labels) if ys_labels is not None else zip(ys, [None] * len(ys))\n        ):\n            plt.magnitude_spectrum(\n                y, Fs=sr, scale=\"dB\", window=np.hanning(len(y)), label=label\n            )\n    if paired_ys is not None:\n        for (f, y), label in (\n            zip(paired_ys, paired_ys_labels)\n            if paired_ys_labels is not None\n            else zip(paired_ys, [None] * len(paired_ys))\n        ):\n            plt.plot(f, 20 * np.log10(np.abs(y)), label=label)\n    plt.xlabel(\"Frequency (Hz)\")\n    plt.ylabel(\"Magnitude (dB)\")\n    plt.xlim(20, sr // 2)\n    plt.title(\"Frequency spectrum\")\n    if ys_labels is not None or paired_ys_labels is not None:\n        plt.legend()"]},{"cell_type":"markdown","metadata":{},"source":["We're going to use a speech sample from the\n[CMU Arctic](http://festvox.org/cmu_arctic/) speech synthesis\ndatabase.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["!wget \"http://festvox.org/cmu_arctic/cmu_arctic/cmu_us_awb_arctic/wav/arctic_a0007.wav\""]},{"cell_type":"markdown","metadata":{},"source":["    --2023-11-01 12:40:09--  http://festvox.org/cmu_arctic/cmu_arctic/cmu_us_awb_arctic/wav/arctic_a0007.wav\n    Resolving festvox.org (festvox.org)... 199.4.150.153\n    Connecting to festvox.org (festvox.org)|199.4.150.153|:80... connected.\n    HTTP request sent, awaiting response... 200 OK\n    Length: 128044 (125K) [audio/x-wav]\n    Saving to: ‘arctic_a0007.wav.2’\n    \n    arctic_a0007.wav.2  100%[===================>] 125.04K   320KB/s    in 0.4s    \n    \n    2023-11-01 12:40:10 (320 KB/s) - ‘arctic_a0007.wav.2’ saved [128044/128044]\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["y, sr = torchaudio.load(\"arctic_a0007.wav\")\ny = y.squeeze()\n\nplt.plot(np.arange(y.shape[0]) / sr, y.numpy())\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Amplitude\")\nplt.show()\n\nAudio(y.numpy(), rate=sr)"]},{"cell_type":"markdown","metadata":{},"source":["![img](42da7dfd6568ad854d8c9b0846645b43c1da8c9a.png)\n\n    <IPython.lib.display.Audio object>\n\nLet's pick up one short segment from the speech, with relatively static\npitch and formants for a stationary model.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["target = y[10000:11024]\n\nfig = plt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplot_t(\"Target signal\", [target.numpy()])\nplt.subplot(1, 2, 2)\nplot_f([target.numpy()], sr=sr)\nplt.show()"]},{"cell_type":"markdown","metadata":{},"source":["![img](34468af2d1255a8fbe7adb11c9c5a9e0190c64ca.png)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Classic LPC Estimation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The common way to estimate the LPC filter is assuming the current sample\n$s[n]$ can only be approximated from past samples. This results in\nminimising the prediction error $e[n]$:\n\n$$\n\\min_{a_k} \\left( s[n] - \\sum_{k=1}^M a_k s[n-k] \\right)^2 = \\min_{a_k} e[n]^2\n$$\n\nIts least squares solution can be computed from the autocorrelation of\nthe signal {cite}=makhoul1975linear=. We'll use the `diffsptk` package\nto compute this.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["lpc_order = 18\nframe_length = 1024\n\nlpc = diffsptk.LPC(lpc_order, frame_length)\ngain, coeffs = lpc(target).split([1, lpc_order], dim=-1)\nprint(f\"Gain: {gain.item()}\")"]},{"cell_type":"markdown","metadata":{},"source":["    Gain: 0.22559228539466858\n\nIf we plot the spectrum of the LPC filter, we'd see that it approximates\nthe spectral envelope of the signal.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["freq_response = (\n    gain\n    / torch.fft.rfft(torch.cat([coeffs.new_ones(1), coeffs]), n=frame_length)\n    / frame_length\n)\n\nfig = plt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplot_t(\"LPC Coefficients\", [coeffs.numpy()], scatter=True, axhline=True, x_label=\"LPC order\")\nplt.ylim(-2, 2)\nplt.subplot(1, 2, 2)\nplot_f(\n    ys=[target.numpy()],\n    ys_labels=[\"target signal\"],\n    paired_ys=[\n        (\n            np.arange(frame_length // 2 + 1) / frame_length * sr,\n            freq_response.numpy(),\n        )\n    ],\n    paired_ys_labels=[\"filter response\"],\n    sr=sr,\n)\nplt.show()"]},{"cell_type":"markdown","metadata":{},"source":["![img](237c3a14dd56149f0d0ae67d914015bdb8ec16df.png)\n\nWe can get the source (or residual) $e[n]$ by inverse filtering the\nsignal with the LPC coefficients, which is equivalent to the filtering\nthe signal with a FIR filter $[1, -a_1, -a_2, \\dots, -a_M]$.\n\n$$\ne[n] = s[n] - \\sum_{k=1}^M a_k s[n-k]\n$$\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["e = (\n    target\n    + F.conv1d(\n        F.pad(target[None, None, :-1], (lpc_order, 0)), coeffs.flip(0)[None, None, :]\n    ).squeeze()\n)\ne = e / gain"]},{"cell_type":"markdown","metadata":{},"source":["After cancelling the spectral envelope, the frequency response of the\nresidual becomes flatter and has very equal energy across the spectrum.\nThis is a result of the least squares optimisation, which assumes that\nthe prediction error is white noise.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["fig = plt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplot_t(\"Residual\", [e.numpy()])\nplt.subplot(1, 2, 2)\nplot_f([e.numpy()], sr=sr)\nplt.show()"]},{"cell_type":"markdown","metadata":{},"source":["![img](a2916696d9903e82fc1a0b5723a148d185c04749.png)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Decomposing Speech with Differentiable LPC and a Glottal Flow Model\n\n"]},{"cell_type":"markdown","metadata":{},"source":["In the above example, we have very little assumptions about the source\n$e[n]$. We only assume that it is whilte-noise like. In the next\nexample, we're going to incorporate a glottal flow model to give more\nconstraints to the source.\n\nThe model we're going to use is the transformed-LF {cite}=fant1995lf=\nmodel, which models the periodic vibration of the vocal folds.\nSpecifically, we're using the derivative of the glottal flow model,\nwhich combines the glottal flow with lips radiation by assuming lips\nradiation is a first-order differentiator. This model has only one\nparameter $R_d$, which is strongly correlated with the perceived vocal\neffort. Although the model is differentiable, for computational\nefficiency, we're going to use a pre-computed lookup table to\napproximate the model.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def transformed_lf(Rd: torch.Tensor, points: int = 1024):\n    # the implementation is adapted from https://github.com/dsuedholt/vocal-tract-grad/blob/main/glottis.py\n    # Ra, Rk, and Rg are called R parameters in glottal flow modeling\n    # We can infer the values of Ra, Rk, and Rg from Rd\n    Rd = torch.as_tensor(Rd).view(-1, 1)\n    Ra = -0.01 + 0.048 * Rd\n    Rk = 0.224 + 0.118 * Rd\n    Rg = (Rk / 4) * (0.5 + 1.2 * Rk) / (0.11 * Rd - Ra * (0.5 + 1.2 * Rk))\n    \n    # convert R parameters to Ta, Tp, and Te\n    # Ta: The return phase duration\n    # Tp: Time of the maximum of the pulse\n    # Te: Time of the minimum of the time-derivative of the pulse\n    Ta = Ra\n    Tp = 1 / (2 * Rg)\n    Te = Tp + Tp * Rk\n\n    epsilon = 1 / Ta\n    shift = torch.exp(-epsilon * (1 - Te))\n    delta = 1 - shift\n\n    rhs_integral = (1 / epsilon) * (shift - 1) + (1 - Te) * shift\n    rhs_integral /= delta\n\n    lower_integral = -(Te - Tp) / 2 + rhs_integral\n    upper_integral = -lower_integral\n\n    omega = torch.pi / Tp\n    s = torch.sin(omega * Te)\n    y = -torch.pi * s * upper_integral / (Tp * 2)\n    z = torch.log(y)\n    alpha = z / (Tp / 2 - Te)\n    EO = -1 / (s * torch.exp(alpha * Te))\n\n    t = torch.linspace(0, 1, points + 1)[None, :-1]\n    before = EO * torch.exp(alpha * t) * torch.sin(omega * t)\n    after = (-torch.exp(-epsilon * (t - Te)) + shift) / delta\n    return torch.where(t < Te, before, after).squeeze()"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["t = torch.linspace(0, 1, 1024)\nplt.plot(t, transformed_lf(0.3).numpy(), label=\"Rd = 0.3\")\nplt.plot(t, transformed_lf(0.5).numpy(), label=\"Rd = 0.5\")\nplt.plot(t, transformed_lf(0.8).numpy(), label=\"Rd = 0.8\")\nplt.plot(t, transformed_lf(2.7).numpy(), label=\"Rd = 2.7\")\nplt.title(\"Transformed LF\")\nplt.legend()\nplt.xlabel(\"T (period)\")\nplt.ylabel(\"Amplitude\")\nplt.show()"]},{"cell_type":"markdown","metadata":{},"source":["![img](fdbbd3593fbb26b5e0e6245ed5f893a9894f81be.png)\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# 0.3 <= Rd <= 2.7 is a reasonable range for Rd\n# we sampled them logarithmically for better resolution at lower values\ntable = transformed_lf(torch.exp(torch.linspace(math.log(0.3), math.log(2.7), 100)))\n\n# align the peaks of the transformed LF for better optimisation\npeaks = table.argmin(dim=-1)\nshifts = peaks.max() - peaks\naligned_table = torch.stack(\n    [torch.roll(table[i], shifts[i].item(), 0) for i in range(table.shape[0])]\n)\n\nplt.title(\"Transformed LF wavetables\")\nplt.imshow(aligned_table, aspect=\"auto\", origin=\"lower\")\nplt.xlabel(\"T (samples)\")\nplt.ylabel(\"Table index\")\nplt.colorbar()\nplt.show()"]},{"cell_type":"markdown","metadata":{},"source":["![img](d408e80f2cf5c84a0225fd96cc9b5cdab5ee7f7b.png)\n\nThe full model we're going to use is:\n\n$$\ns[n] = g \\cdot w\\left((\\frac{n f_0}{f_s} + \\phi)  \\mod 1; R_d \\right) + \\sum_{k=1}^M a_k s[n-k].\n$$\n\nWe replace source $e[n]$ with the following parameters: gain $g$,\nfundamental frequency $f_0$, phase offset $\\phi$, and $R_d$. $w$\nis the pre-computed glottal flow model, and $f_s$ is the sampling\nrate. Let's define this model in code.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["class SourceFilter(torch.nn.Module):\n    def __init__(\n        self,\n        lpc_order: int,\n        sr: int,\n        table_points=1024,\n        num_tables=100,\n        init_f0: float = 100.0,\n        init_offset: float = 0.0,\n        init_log_gain: float = 0.0,\n    ):\n        super().__init__()\n\n        Rd_sampled = torch.exp(torch.linspace(math.log(0.3), math.log(2.7), num_tables))\n        table = transformed_lf(Rd_sampled, points=table_points)\n        peaks = table.argmin(dim=-1)\n        shifts = peaks.max() - peaks\n        aligned_table = torch.stack(\n            [torch.roll(table[i], shifts[i].item(), 0) for i in range(table.shape[0])]\n        )\n        self.register_buffer(\"table\", aligned_table)\n        self.register_buffer(\"Rd_sampled\", Rd_sampled)\n\n        self.f0 = torch.nn.Parameter(torch.tensor(init_f0))\n        self.offset = torch.nn.Parameter(torch.tensor(init_offset))\n        self.Rd_index_logits = torch.nn.Parameter(torch.zeros(1))\n        self.log_gain = torch.nn.Parameter(torch.tensor(init_log_gain))\n\n        # we use the reflection coefficients parameterisation for stable optimisation\n        self.log_area_ratios = torch.nn.Parameter(torch.zeros(lpc_order))\n        self.logits2lpc = torch.nn.Sequential(\n            diffsptk.LogAreaRatioToParcorCoefficients(lpc_order),\n            diffsptk.ParcorCoefficientsToLinearPredictiveCoefficients(lpc_order),\n        )\n\n        self.lpc_order = lpc_order\n        self.table_points = table_points\n        self.num_tables = num_tables\n        self.sr = sr\n\n    @property\n    def Rd_index(self):\n        return torch.sigmoid(self.Rd_index_logits) * (self.num_tables - 1)\n\n    @property\n    def Rd(self):\n        return self.Rd_sampled[torch.round(self.Rd_index).long().item()]\n\n    @property\n    def gain(self):\n        return torch.exp(self.log_gain)\n\n    @property\n    def filter_coeffs(self):\n        return self.logits2lpc(\n            torch.cat([self.log_gain.view(1), self.log_area_ratios])\n        ).split([1, self.lpc_order])\n\n    def source(self, steps):\n        \"\"\"\n        Generate the gloottal pulse source signal\n        \"\"\"\n\n        # select the wavetable using linear interpolation\n        select_index_floor = self.Rd_index.long().item()\n        p = self.Rd_index - select_index_floor\n        selected_table = (\n            table[select_index_floor] * (1 - p) + table[select_index_floor + 1] * p\n        )\n\n        # generate the source signal by interpolating the wavetable\n        phase = (\n            torch.arange(\n                steps, device=selected_table.device, dtype=selected_table.dtype\n            )\n            / self.sr\n            * self.f0\n            + self.offset\n        ) % 1\n        phase_index = phase * self.table_points\n        # append the first sample to the end for easier interpolation\n        padded_table = torch.cat([selected_table, selected_table[:1]])\n        phase_index_floor = phase_index.long()\n        phase_index_ceil = phase_index_floor + 1\n        p = phase_index - phase_index_floor\n        glottal_pulse = (\n            padded_table[phase_index_floor] * (1 - p)\n            + padded_table[phase_index_ceil] * p\n        )\n        return glottal_pulse\n\n    def forward_filt(self, e):\n        \"\"\"\n        Apply the LPC filter to the input signal\n        \"\"\"\n        # get filter coefficients\n        log_gain, lpc_coeffs = self.filter_coeffs\n\n        # IIR filtering\n        b = log_gain.new_zeros(1 + lpc_coeffs.shape[-1])\n        b[0] = torch.exp(log_gain)\n        a = torch.cat([lpc_coeffs.new_ones(1), lpc_coeffs])\n        return lfilter(e, a, b, clamp=False)\n\n    def forward(self, steps):\n        \"\"\"\n        Generate the speech signal\n        \"\"\"\n        return self.forward_filt(self.source(steps))\n\n    def inverse_filt(self, s):\n        \"\"\"\n        Inverse filtering\n        \"\"\"\n        # get filter coefficients\n        _, lpc_coeffs = self.filter_coeffs\n\n        e = (\n            s\n            + F.conv1d(\n                F.pad(s[None, None, :-1], (self.lpc_order, 0)),\n                lpc_coeffs.flip(0)[None, None, :],\n            ).squeeze()\n        )\n        e = e / self.gain\n        return e"]},{"cell_type":"markdown","metadata":{},"source":["Proper initialisation of the parameters plays an important role in the\noptimisation. We're going to use the following initialisation.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["model = SourceFilter(lpc_order, sr, init_f0=130.0, init_offset=0.0, init_log_gain=-1.3)\nprint(f\"Gain: {model.gain.item()}\")\nprint(f\"Rd: {model.Rd.item()}\")\nprint(f\"f0: {model.f0.item()}\")\nprint(f\"Offset: {model.offset.item() % 1}\")"]},{"cell_type":"markdown","metadata":{},"source":["    Gain: 0.27253180742263794\n    Rd: 0.9100430011749268\n    f0: 130.0\n    Offset: 0.0\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["with torch.no_grad():\n    output = model(1024)\n\n\nfig = plt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplot_t(\"Initial prediction\", [output.numpy(), target.numpy()], labels=[\"predict (initial)\", \"target\"])\nplt.subplot(1, 2, 2)\nplot_f(\n    ys=[output.numpy(), target.numpy()],\n    ys_labels=[\"predict (initial)\", \"target\"],\n    sr=sr,\n)\nplt.show()"]},{"cell_type":"markdown","metadata":{},"source":["![img](1b4e859c8c2846cfbf538a2a077bedd5d30699f2.png)\n\nLet's optimise the parameters with gradient descent. We're going to use\nthe famous Adam optimiser with a learning rate of 0.001 and run it for\n2000 iterations. The loss function we're going to use is the L1 loss\nbetween the original signal and the modelled signal.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nlosses = []\nfor _ in range(2000):\n    optimizer.zero_grad()\n    output = model(1024)\n    loss = F.l1_loss(output, target)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n\nplt.plot(losses)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.show()"]},{"cell_type":"markdown","metadata":{},"source":["![img](ebc2fb7b092fa47b0e5149a410daaa718e196b0a.png)\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["with torch.no_grad():\n    final_output = model(1024)\n\nprint(f\"Gain: {model.gain.item()}\")\nprint(f\"Rd: {model.Rd.item()}\")\nprint(f\"f0: {model.f0.item()}\")\nprint(f\"Offset: {model.offset.item() % 1}\")\n\nfig = plt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplot_t(\"Final prediction\", [final_output.numpy(), target.numpy()], labels=[\"predict (optimised)\", \"target\"])\nplt.subplot(1, 2, 2)\nplot_f(\n    ys=[final_output.numpy(), target.numpy()],\n    ys_labels=[\"predict (optimised)\", \"target\"],\n    sr=sr,\n)\nplt.show()"]},{"cell_type":"markdown","metadata":{},"source":["    Gain: 0.18433158099651337\n    Rd: 1.5502203702926636\n    f0: 131.0266876220703\n    Offset: 0.9483504593372345\n\n![img](7665265484c1fc854d600f11d6fe67a6ed9da3aa.png)\n\nWow, this is pretty good! We can see that the model reconstructs the\noriginal signal quite well with very similar waveforms. Moreover, the\nmodel tells what are the optimal parameters to construct the source\nsignal. Let's see what is the source signal looks like.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["with torch.no_grad():\n    e = model.source(1024)\n    s = model.forward_filt(e)\n\nfig = plt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplot_t(\"Waveform\", [e.numpy() / 4, s.numpy()], labels=[\"e[n]\", \"s[n]\"])\nplt.subplot(1, 2, 2)\nplot_f(\n    ys=[e.numpy() / 4, s.numpy()],\n    ys_labels=[\"e[n]\", \"s[n]\"],\n    sr=sr,\n)\nplt.show()"]},{"cell_type":"markdown","metadata":{},"source":["![img](97c939e40ca376580564bb3c5a7ef54d6f3dccfe.png)\n\nLet's compare the spectrum of the two filters.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["_, lpc_coeffs = model.filter_coeffs\nwith torch.no_grad():\n    freq_response_opt = (\n        model.gain\n        / torch.fft.rfft(\n            torch.cat([lpc_coeffs.new_ones(1), lpc_coeffs]), n=frame_length\n        )\n        / frame_length\n    )\n\n\nfig = plt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplot_t(\n    \"LPC Coefficients\",\n    [coeffs.numpy(), lpc_coeffs.detach().numpy()],\n    labels=[\"least squares LPC\", \"differentiable LPC\"],\n    scatter=True,\n    axhline=True,\n    x_label=\"LPC order\",\n)\nplt.ylim(-2, 2)\nplt.subplot(1, 2, 2)\nfreqs = np.arange(frame_length // 2 + 1) / frame_length * sr\nplot_f(\n    paired_ys=[\n        (\n            freqs,\n            freq_response.numpy(),\n        ),\n        (\n            freqs,\n            freq_response_opt.numpy(),\n        ),\n    ],\n    paired_ys_labels=[\"least squares LPC\", \"differentiable LPC\"],\n    sr=sr,\n)\nplt.show()"]},{"cell_type":"markdown","metadata":{},"source":["![img](d0cb68d01e44e769f66d02d95095da95632e50cb.png)\n\nInterestingly, the two filters looks very different. The biggest reason\nis because we restricted the source signal to have specific shapes. The\ngradient method also can not achieve a lossless decomposition, while the\nclassic LPC method can. However, the source signal we get from the\ngradient method is much more interpretable. In fact, the latter method\nis a simplified version of the synthesiser used in GOLF vocoder proposed\nby Yu [&Yu-2023](&Yu-2023).\n\n[cerkut.bib](cerkut.bib)\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}